# Image captioning

Проект по построению и практическому использованию модели автоматической 
текстовой аннотации
изображений для поиска непохожих моментов в видео.

## Идея

1. С помощью нейронной сети создается аннотация кадра из видео
2. Аннотация сравнивается с аннотациями предыдущих кадров видео и считается
некоторая метрика непохожести текущего кадра на предыдущие

## Имплементация

1. Для логирования результатов экспериментов
(разные размерности эмбеддингов слов, скрытых слоев и т.д.) и соответствующих
кодов использовался [neptune](https://neptune.ai/)
2. Модель, аннотирующая изображения была написана с нуля на pytorch, 
в качестве архитектуры 
использовалась модель с soft attention из статьи 
[Show, Attend and Tell](https://arxiv.org/abs/1502.03044).
3. В качестве датасета использоваться 
[Microsoft COCO](https://cocodataset.org/#home)
3. Изображения кодировались с помощью Resnet 152 и 
затем подавались в однонаправленную LSTM
4. Для создания интерфейса была использована следующая связка:
- flask сервер с моделью, обрабатывающий все запросы и 
возвращающий текстовые описания, закодированные сверточной сетью
изображения и эмбеддинги слов из текстовых описаний.
- shiny app для создания интерфейса
- flask сервер и shiny app завернуты в докер контейнеры
- докер контейнеры поднимаются с помощью docker compose up, 
с уже прописанным пробросом необходимых портов


## Результаты

При обучении модели, аннотирующей изображения, удалось достичь BLEU4 = 20.14.
При этом для многих изображений модель выдает адекватные описания\
Примеры:
1. A group of cows are grazing in a field
![A group of cows are grazing in a field](https://sun9-31.userapi.com/impg/pb6wXXMsVoh1DtPo_pE-3ZnDOSskyLQ5PkjAgw/z7pmZYR-EcM.jpg?size=2560x1440&quality=96&sign=d9f1451eefca4918d0945d0f453a5ca3&type=album)
2. A city street with a car parked on the side of the road
![A city street with a car parked on the side of the road](https://sun9-59.userapi.com/impg/7zIOiTroHQiYArLx_c3_jpCMlN8D_RHd5T-1Mw/iLM1FbDC9eU.jpg?size=913x567&quality=96&sign=c5e4ceb6165880a72fa19658c077dad3&type=album)

При этом для изображений под углом часто получаются плохие описания. Это происходит, потому что модель
была обучена на изображениях, снятых параллельно земле.

Пример:
1. A train on a track with a train on it
![A train on a track with a train on it](https://sun9-41.userapi.com/impg/wEHEAlN7J0IT6SNXBmZK8skNKrmTHqc578qZ1A/5sQR7A9pyHQ.jpg?size=1920x1080&quality=96&sign=689f532cab1c5fb9a3c9a30ee5d59078&type=album)


При анализе видео для нахождения непохожих друг на друга кадров 
используются не сами слова из текстовых аннотаций, а эмбеддинги слов, т.к. 
для большинства кадров слова в описании не повторяются с предыдущим кадром,
поэтому текстовый анализ невозможен. Эмбеддинги же представляют собой вектор,
при этом обладающий смыслом (king - man + women = queen и тд). Поэтому,
для похожих кадров, имеющих разное описание, эмбеддинги слов все равно близки друг к
другу.